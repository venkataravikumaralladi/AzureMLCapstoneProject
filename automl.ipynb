{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "In this capstone project I will showcase how we can use data science as a investigation tool, here we use classfication algorithm to distinguish between normal traffic (good connections) and intrusion or attacks traffic (bad connections). A connection is a sequence of TCP packets starting and ending at some well difined times, between which data flows to and from source IP address to a target IP address under some well defined protocol. We will create Intrusion Detection System (IDS)\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an `Experiment` in an existing `Workspace`.\n",
    "2. Create or Attach existing AmlCompute to a workspace.\n",
    "3. Define data loading in a `TabularDataset`.\n",
    "4. Configure AutoML using `AutoMLConfig`.\n",
    "5. Use AutoMLStep\n",
    "6. Train the model using AmlCompute\n",
    "7. Explore the results.\n",
    "8. Test the best fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies.\n",
    "In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Data description\n",
    "Data is collected by packet analyzers (also known as packet/network/protocol snifers) intercept and log traffic in the network.The dataset that we will use is the NSLKDD dataset. The original 1999 KDD Cup dataset was created for the DARPA Intrusion Detection Evaluation Program, prepared and managed by MIT Lincoln Laboratory. The data was collected over nine\n",
    "weeks and consists of raw tcpdump traffic in a local area network (LAN) that simulates the environment of a typical United States Air Force LAN. Some network attacks were deliberately carried out during the recording period. There were 38 different\n",
    "types of attacks, but only 24 are available in the training set. \n",
    "\n",
    "#### Data references\n",
    "\n",
    "https://www.unb.ca/cic/datasets/nsl.html     \n",
    "https://www.kaggle.com/hassan06/nslkdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "vrk_auto_ids_ws = Workspace.from_config()\n",
    "vrk_auto_ids_exp = Experiment(workspace=vrk_auto_ids_ws, name=\"vrk_ids_auto_exp\")\n",
    "\n",
    "print('Workspace name: ' + vrk_auto_ids_ws.name, \n",
    "      'Azure region: ' + vrk_auto_ids_ws.location, \n",
    "      'Subscription id: ' + vrk_auto_ids_ws.subscription_id, \n",
    "      'Resource group: ' + vrk_auto_ids_ws.resource_group, sep = '\\n')\n",
    "\n",
    "auto_ids_run = vrk_auto_ids_exp.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach an AmlCompute cluster\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=vrk_ids_ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(vrk_ids_ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and prepare data for AutoML learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "\n",
    "# Create TabularDataset using TabularDatasetFactory\n",
    "\n",
    "kdd_auto_webpath = [\n",
    "                            'https://raw.githubusercontent.com/venkataravikumaralladi/AzureMLCapstoneProject/main/KDDTrain.csv'\n",
    "                            ]\n",
    "\n",
    "#create bankmarketing data set in tabular format using TabularDatasetFactory\n",
    "ids_auto_dataset = TabularDatasetFactory.from_delimited_files(path=kdd_auto_webpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class variables\n",
    "network_data_column_names = [ \n",
    "                  'duration', 'protocol_type', 'service',\n",
    "                  'flag', 'src_bytes', 'dst_bytes',\n",
    "                  'land', 'wrong_fragment', 'urgent',\n",
    "    \n",
    "            \n",
    "                  'hot', 'num_failed_logins', 'logged_in',\n",
    "                  'num_compromised', 'root_shell', 'su_attempted',\n",
    "                  'num_root', 'num_file_creations', 'num_shells',\n",
    "                  'num_access_files', 'num_outbound_cmds', 'is_hot_login',\n",
    "                  'is_guest_login',\n",
    "    \n",
    "                 \n",
    "                  'count', 'srv_count', 'serror_rate',\n",
    "                  'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "                  'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "                 \n",
    "                  'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "                  'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "                  'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "                  'dst_host_srv_rerror_rate',\n",
    "    \n",
    "                   'attack_type',\n",
    "                   'success_pred' ]\n",
    "\n",
    "train_df = ids_auto_dataset.to_pandas_dataframe().dropna()\n",
    "train_df.columns = network_data_column_names\n",
    "\n",
    "# For this analysis we drop \"success_pred\" column\n",
    "train_df.drop('success_pred', axis=1, inplace=True)\n",
    "    \n",
    "# Drop attack type in training data which is to be predicted.\n",
    "train_X = train_df.drop(\"attack_type\", axis=1)\n",
    "train_Y = train_df['attack_type']\n",
    "\n",
    "from train import clean_data\n",
    "\n",
    "# Use the clean_data function to clean your data.\n",
    "x, y = clean_data(ids_auto_dataset)\n",
    "x = x.join(y)\n",
    "x.head()\n",
    "\n",
    "#store data frame to data store for AutoMLConfig training\n",
    "x.to_csv('training/bank_training_data.csv')\n",
    "default_datastore = vrk_bank_ws.get_default_datastore()\n",
    "default_datastore\n",
    "\n",
    "#upload training data to data store for AutomMLConfig training\n",
    "default_datastore.upload(src_dir='training', target_path='data/')\n",
    "\n",
    "# get data set pointer to data store for bank training data\n",
    "from azureml.core import Dataset\n",
    "banktraining_dataset = Dataset.Tabular.from_delimited_files(path=[(default_datastore, ('data/bank_training_data.csv'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "# Set parameters for AutoMLConfig\n",
    "# NOTE: DO NOT CHANGE THE experiment_timeout_minutes PARAMETER OR YOUR INSTANCE WILL TIME OUT.\n",
    "# If you wish to run the experiment longer, you will need to run this notebook in your own\n",
    "# Azure tenant, which will incur personal costs.\n",
    "vrk_bnk_automl_config = AutoMLConfig(\n",
    "                   experiment_timeout_minutes=30,\n",
    "                   task=\"classification\",\n",
    "                   primary_metric=\"accuracy\",\n",
    "                   compute_target = cpu_cluster,\n",
    "                   training_data=banktraining_dataset,\n",
    "                   label_column_name='y',\n",
    "                   n_cross_validations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Submit your automl run\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "vrk_auto_bank_exp = Experiment(workspace=vrk_bank_ws, name=\"vrk_auto_bank_train_exp\")\n",
    "automl_bnk_run = vrk_auto_bank_exp.submit(vrk_bnk_automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(automl_bnk_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_bnk_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(automl_bnk_run.get_status() == \"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve and save your best automl model.\n",
    "best_auto_run, best_fitted_model = automl_bnk_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "best_fitted_model._final_estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_auto_run.register_model(model_name='vrk_bank_best_auto_model_predictor', model_path='./outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
